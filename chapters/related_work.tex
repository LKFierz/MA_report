\chapter{Related Work}
\label{sec:relwork}


Autonomous safe landing is perhaps the most important part of a rotorcraft's mission. Therefore, it is no surprise that tremendous work has been accomplished to achieve this crucial feature. 

Camera sensors are highly advantageous for navigation due to their lightweight nature and the extensive research dedicated to their development over the years. Their minimal weight makes them particularly suitable for applications where payload capacity is critical, such as rotorcraft Mars missions. Decades of intensive research have culminated in highly sophisticated algorithms and methodologies that leverage the rich data captured by visual sensors and enable the daunting task of autonomous landing.

In the following, previous work in this field is introduced and discussed with consideration of our mission goal. Most approaches split the task of autonomous landing into a visual step that analyses the terrain or the surroundings, a selection task that determines the location to land at, and finally, a navigation part that considers the selected location and approaches it.

\section{Landing based on Visual Markers}
\citep{Saripalli2002VisionBasedLanding,Falanga2017QuadrotorLanding} and \citep{Mu2023VisionBasedLanding} use artificial landing markers as indications of valid landing sites. 

\begin{itemize}
    \item \citet{Saripalli2002VisionBasedLanding} lands on a fixed landing pad using GPS and a camera. The pad is marked using a distinctive, geometric visual marking. The pad is recognized by leveraging the geometrical shape of the marker. The calibration of this recognition algorithm, distinguishing the landing marker from other high-frequency objects, was done using information from previous flights. After detecting the marker, the x-y coordinates and orientation of the center point are derived, and the drone navigates towards that location. The reactive high-level landing behavior is implemented using a state machine.
    \item \citet{Falanga2017QuadrotorLanding} introduce a method to land on moving platforms. They work with visual markers on the platform and utilize onboard sensors and visual-inertial state estimation to localize and pursue this visual marker. An extended Kalman Filter estimates the platform's motion, including position, velocity, and orientation. The high-level decisions are again made using a state machine with four states: takeoff, exploration, platform tracking, and landing. 
    \item \citet{Mu2023VisionBasedLanding} use an off-the-shelf commercial drone with a camera tilted 45 degrees. They added a lens to this camera to achieve a downside view and detect the marker. The pad detection and localization are achieved by running the deep learning-based YOLOv5 algorithm on a ground station. The connection between the drone and the ground station is established using a wifi connection. The platform tracking behavior is orchestrated using a state machine. It consists of an ascent to an overview altitude, the detection and localization of the marked platform, the navigation towards the platform with reactive steps for the case when the pad is lost out of sight, and the landing mechanism.
\end{itemize}

Visual markers are a great tool for efficiently detecting a valid landing area. However, they are not usable for our endeavor, as we are flying on Mars' uncharted terrain. Once valid landing sites are detected, the predominantly used approach of pursuing the chosen landing zone using visual sensors orchestrated by a state machine and using a vision-based on-board state estimator is a perfectly valid strategy for our mission as well.


\section{Homography-based Autonomous Landing}

\citep{Bosch2006AutonomousDetection,Brockers2011AutonomousLanding,Desaraju2015VisionBased} and \citep{Brockers2014TowardsAutonomous} pursue autonomous landing implementations based on homography assumptions: 

\begin{itemize}
    \item \citet{Bosch2006AutonomousDetection} use homography estimates and feature correspondences to continuously update a model of the surroundings. This way, they can determine horizontal, obstacle-free areas. 
    \item \citet{Brockers2011AutonomousLanding} exploit a planarity assumption of the scene to achieve landing on elevated platforms and ingress through rectangular openings. No visual markers are used. Using SURF-feature point correspondences, homography estimates are created. To refine these estimates, further surfaces are used to apply a multiple-homography-alignment-algorithm. Lastly, the calculated plane normal and the estimated vehicle motion parameters are used to find a point located on the plane. This visual algorithm is split into four phases: takeoff, plane detection, refinement, and descent.
\end{itemize}

Though a valid procedure to find adequate, flat landing areas using homography-based approaches is unfeasible for our purposes, as a homography assumption can't generally be made on Mars' rough and obstacle-rich terrain.  


\section{Landing Mechanisms based on Range Sensors}

\begin{itemize}
    \item \citet{Johnson2002LidarBased} use a LiDAR sensor for 3D reconstruction of surrounding terrain for a larger Mars lander vehicle. The LiDAR data is aggregated into an elevation map. On that map, landing areas are determined based on an assessment of their slope and roughness. In a subsequent step, the landing guidance algorithm navigates toward the currently chosen landing sites. This guidance module is designed to redirect the current pathing to another landing site should incoming measurements change the optimal landing site candidate.
    \item \citet{Scherer2012AutonomousLanding} introduce a landing pipeline for a full-scale helicopter. LiDAR measurements are analyzed in a coarse-to-fine manner. This way, they can pre-determine promising landing areas which can subsequently be assessed in more detail. The defining characteristics of a landing site are the slope of a fitted plane and the roughness.
\end{itemize}

Weight is a limiting constraint for our endeavour, as a rotorcraft has to fly on Mars' 1\% air density. Thus, a LiDAR sensor that easily weighs half a kilogram is not a valid choice. Cameras, on the other hand, can weigh as little as a few grams.

\section{Mars Lander Approach}

In \citep{Johnson2020Mars2020}, NASA pursued a vision-based strategy using a predefined map of Mars' surface and a downwards-facing monocular camera to orient a Mars lander in the surrounding terrain. However, rotorcrafts need to consider much smaller hazards compared to a lander. At 25 cm/pixel for images and 1 m/pixel for the DEM, the available Mars footage from the HiRISE camera on the Mars Reconnaissance Orbiter is not sufficient in resolution to supply prior information to the landing process of a UAV. Rover images and Ingenuity footage could be used. However, the usage of this data would significantly limit possible flight areas.


\section{Monocular Stereo-based Autonomous Landing}

\begin{itemize}
    \item \citet{Desaraju2015VisionBased} use monocular images and the drone's lateral movement to reconstruct the terrain. Thereafter, all the terrain information farther away than a certain threshold is disregarded, and safe landing areas are detected on the elevation information. The criteria for selecting landing sites include the flatness and levelness of the area, ample space for the UAV to land, and the absence of obstacles.
    \item \citet{Brockers2014TowardsAutonomous} use two vision-based approaches for state estimation. First, the onboard camera is used as a velocity sensor by applying an inertial optical flow algorithm. This allows the drone to very quickly stabilize when thrown into the air. Secondly, visual self-localization and mapping are used to estimate the pose of the drone over a longer duration with significantly less accumulated drift. Both of these measurements and IMU readings are merged into an Extended Kalman Filter. All of this is processed on an onboard processing unit. For autonomous landing, the same approach as in \citep{Desaraju2015VisionBased} is applied using monocular camera images to reconstruct the surroundings and choose a high-vantage landing plane without obstacles and of sufficient size.
\end{itemize}

Both \citep{Desaraju2015VisionBased} and \citep{Brockers2014TowardsAutonomous} use monocular stereo to reconstruct the terrain and detect landing sites on it. The problem of detecting landing sites on the direct stereo depth output is the generated depth errors. A possible remedy for this is elevation maps.

\section{Elevation Mapping Approaches}
 Other modern approaches make use of 2.5D representations of the surroundings. While \citep{Johnson2005VisionGuided} uses a fixed surface elevation map, \citep{Fankhauser2014RobotCentric} and \citep{Daftry2018Robust} use a robot-centric 2.5D terrain representation similar to the setup used in this project.

\begin{itemize}
    \item \citet{Johnson2005VisionGuided} uses monocular camera images and a structure from motion algorithm to create point clouds. These point clouds are converted into a fixed surface frame and aggregated in a dense elevation map (DEM). Hazard segmentation is performed using the common strategy of determining the slope on the map by fitting planes and considering smaller obstacles like rocks. A binary landing map is created based on these two metrics and a distance transform to ensure the minimally necessary size of a valid spot. Lastly, the site with the largest circular valid patch is selected. If two sites compete at the same level, the one closer to the pre-determined emergency landing location is chosen.
    \item \citet{Fankhauser2014RobotCentric} use a forward-looking range sensor and onboard state estimation to create a local, robot-centric elevation map used for intermediate path planning within a global mission. The map is implemented as a 2D grid where each cell stores a Gaussian cell state containing a height estimate and a variance. To account for the drift accumulated in the robot's pose, the uncertainty in the local map's uncertainty is grown based on the covered distance and orientation changes. This approach does not feed information from the created local map back to state estimation.
    \item \citet{Daftry2018Robust} use thermal-infrared cameras and a visual-inertial odometry algorithm to localize the UAV in its surroundings during nighttime. They apply a monocular depth estimation approach matching pixel intensities directly. These are aggregated in the 2.5D elevation map outlined in \citep{Fankhauser2014RobotCentric}. As in previous approaches, landing sites are detected on the created DEM according to slope, roughness, and size of a landing spot.
\end{itemize}


\section{Learning based Methods}
Other novel approaches use learning-based methods as did \citep{Neves2024Multimodal, Abdollahzadeh2022SafeLandingZones} and \citep{TovanchePicon2024RealTimeSafeValidation}. 

\begin{itemize}
    \item \citet{Abdollahzadeh2022SafeLandingZones} uses a deep regression approach in the shape of a u-net to solve the challenge of detecting and scoring landing sites. The input to the network is an RGB image which is projected into a lower-dimensional latent space. From there, the intermediate representation is up-sampled again to finally output a landing site score map revolving around three main levels with smooth transitions in between - low risk, medium risk, and high risk.
    \item \citet{Neves2024Multimodal} counteract traditional sensor limitations for detecting marked landing sites by implementing a deep learning-based multimodal sensor fusion network capable of merging 3D LiDAR data, thermal, and ordinary camera images. The network consists of a backbone containing a block of CNNs for feature extraction, a transformer encoder, and two multi-layer perceptions, which output the two outputs which are a confidence score that a landing site indicated by a landing marker is present in a certain bounding box and the coordinates of said bounding box. Secondly, they introduce a reinforcement learning-based decision-making lander. This lander consists of multiple layers that, in the end, output the correct landing action. The action space is limited to moving forward, backward, left, right, and vertical descent. The reward function consists of a weighted consideration of each dimension's distance to the landing platform. It was trained on simulated data and transferred into the real world.
\end{itemize}

As deep learning methods have significantly outperformed classical computer vision methods in other applications, they are certainly promising and, in the long run, definitely a pathway to consider. In addition to great general performance, deep learning methods provide significant robustness to a change in sensor information, such as camera images taken at different times throughout the day. 

However, learning-based methods come with significant drawbacks in the context of the pursued task. First, considering the limitations present in Mars missions, the probable additional computational overhead from learning-based methods can not be neglected. Furthermore, neural network-based solutions give up simplicity and interpretability for the benefit of precision. This is not to be underestimated in a hostile environment such as Mars' rough terrain, where perfect accuracy and as much information for mission planning purposes as possible are required. Additionally, learning-based methods require substantial training data, which is not available in large quantities in the context of autonomous UAV landing. Especially when the training data needs to come from another planet. Lastly, missions flown on Mars pose unique challenges compared to conventional flights. 

Note that the previously laid out \citet{Mu2023VisionBasedLanding} uses YOLOv5, a deep-learning-based detection approach for the detection of the landing marker. It is no surprise that in their setup this was run on a ground station as opposed to the processing unit on the drone.